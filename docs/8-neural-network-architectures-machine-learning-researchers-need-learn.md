# 机器学习研究人员需要学习的 8 种神经网络架构

> 原文：[https://www.kdnuggets.com/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html/2](https://www.kdnuggets.com/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html/2)

![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html?page=2#comments)

### 5 — 霍普菲尔德网络

非线性单元的递归网络通常很难分析。它们可以表现出许多不同的方式：稳定到一个稳定状态、振荡或沿着无法预测的混沌轨迹前进。一个**霍普菲尔德网络**由二进制阈值单元组成，这些单元之间有递归连接。1982年，[约翰·霍普菲尔德](http://www.pnas.org/content/79/8/2554.full.pdf)意识到，如果连接是对称的，就存在一个全局能量函数。整个网络的每个二进制“配置”都有一个能量；而二进制阈值决策规则使网络趋向于这个能量函数的最小值。利用这种计算类型的一种巧妙方式是将记忆作为神经网络的能量最小值。使用能量最小值来表示记忆提供了一种内容可寻址的记忆。只需知道其内容的一部分即可访问该项。它对硬件损坏具有鲁棒性。

![](../Images/389e610dd01eba5d37d2433e8cf8f61a.png)

每次我们记住一个配置时，我们希望创建一个新的能量最小值。但是，如果两个相邻的最小值位于一个中间位置怎么办？这限制了霍普菲尔德网络的容量。那么我们如何增加霍普菲尔德网络的容量呢？物理学家喜欢这样一个想法：他们已经知道的数学可能解释大脑的工作原理。许多论文在物理学期刊上发表，讨论霍普菲尔德网络及其存储容量。最终，[伊丽莎白·加德纳](http://www.baginsky.de/eli/eg_portr.html)发现有一个更好的存储规则，能够充分利用权重的容量。她没有试图一次性存储向量，而是多次循环遍历训练集，并使用感知机收敛过程来训练每个单元，以便在给定该向量中所有其他单元的状态的情况下具有正确的状态。统计学家称这种技术为“伪似然”。

![](../Images/a5879ce459a4610c5864a73590815152.png)

霍普菲尔德网络还有另一种计算作用。我们可以用它来构建对感官输入的解释，而不是用来存储记忆。输入由可见单元表示，解释由隐藏单元的状态表示，而解释的糟糕程度由能量表示。

### 6 — 玻尔兹曼机网络

**玻尔兹曼机** 是一种随机递归神经网络。它可以看作是Hopfield网的随机生成对等物。它是最早能够学习内部表示的神经网络之一，能够表示和解决困难的组合问题。

![](../Images/b30a0c0fdfe5954adf36bd121d128e76.png)

玻尔兹曼机学习算法的学习目标是最大化玻尔兹曼机对训练集中的二进制向量分配的概率的乘积。这等同于最大化玻尔兹曼机对训练向量分配的对数概率之和。这也等同于最大化我们得到准确N个训练样本的概率，如果我们进行以下操作：1）让网络在没有外部输入的情况下收敛到其稳态分布N次；2）每次采样一次可见向量。

Salakhutdinov 和 Hinton 在2012年提出了一种高效的小批量学习程序用于玻尔兹曼机，详见 [Salakhutdinov 和 Hinton 在2012](http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf)。

+   对于正向阶段，首先将隐藏单元的概率初始化为0.5，然后将数据向量固定在可见单元上，然后使用均值场更新并行更新所有隐藏单元直到收敛。在网络收敛后，记录每一对连接单元的PiPj，并对小批量中的所有数据取平均。

+   对于负向阶段：首先保持一组“虚拟粒子”。每个粒子都有一个作为全局配置的值。然后依次更新每个虚拟粒子中的所有单元几次。对于每一对连接单元，计算SiSj在所有虚拟粒子中的平均值。

在一般的玻尔兹曼机中，单位的随机更新需要按顺序进行。有一种特殊的架构允许交替的并行更新，这种更新方式更高效（层内没有连接，层间没有跳跃连接）。这种小批量处理使玻尔兹曼机的更新更加并行。这被称为深度玻尔兹曼机（DBM），它是一个具有许多缺失连接的通用玻尔兹曼机。

![](../Images/2fb367be246b0e42e30f3f464619de56.png)

2014年，Salakhutdinov 和 Hinton 提出了对其模型的另一种更新，称为 [限制玻尔兹曼机](http://www.machinelearning.org/proceedings/icml2007/papers/407.pdf)。他们限制了连接性以简化推理和学习（仅有一层隐藏单元且隐藏单元之间没有连接）。在RBM中，当可见单元被固定时，只需一步即可达到热平衡。

另一种高效的小批量学习程序如下：

+   对于正向阶段，首先将数据向量固定在可见单元上。然后计算每对可见单元和隐藏单元的<ViHj>的确切值。对于每一对连接单元，计算<ViHj>在小批量数据中的平均值。

+   对于负阶段，也保持一组“虚拟粒子”。然后使用交替并行更新更新每个虚拟粒子几次。对于每对连接的单元，平均所有虚拟粒子的 ViHj。

### 7 — 深度信念网络

![](../Images/40b880c73edf17c5116730a46ce96a27.png)

反向传播被认为是人工神经网络中计算每个神经元在处理一批数据后错误贡献的标准方法。然而，使用反向传播存在一些主要问题。首先，它需要标记的训练数据，而几乎所有的数据都是未标记的。其次，学习时间不具备良好的扩展性，这意味着在具有多个隐藏层的网络中，它非常缓慢。第三，它可能会陷入较差的局部最优解，因此对于深度网络，它们离最优解还很远。

为了克服反向传播的局限性，研究人员考虑使用无监督学习方法。这有助于保持使用梯度方法调整权重的效率和简便性，同时还用于建模感官输入的结构。特别是，他们调整权重以最大化生成模型生成感官输入的概率。问题是我们应该学习什么样的生成模型？它可以是类似于玻尔兹曼机的基于能量的模型吗？或者是由理想化神经元组成的因果模型？还是两者的混合？

![](../Images/9309979c829915136afaa38be8b8b931.png)

**信念网络** 是由随机变量组成的有向无环图。使用信念网络，我们可以观察到一些变量，并希望解决两个问题：1）推理问题：推断未观察到的变量的状态；2）学习问题：调整变量之间的交互，使网络更有可能生成训练数据。

早期的图形模型使用专家定义图的结构和条件概率。那时，图的连接稀疏；因此，研究人员最初专注于进行正确的推理，而不是学习。对于神经网络，学习是核心，手工编写知识并不可取，因为知识来自于学习训练数据。神经网络并不以可解释性或稀疏连接为目标来简化推理。尽管如此，信念网络也有神经网络版本。

有两种类型的生成神经网络，由随机二进制神经元组成：1）**基于能量的**，其中我们使用对称连接连接二进制随机神经元以获得玻尔兹曼机；2）**因果**，其中我们在有向无环图中连接二进制随机神经元以获得 sigmoid 信念网络。这两种类型的描述超出了本文的范围。

### 8 — 深度自编码器

![](../Images/a3146c500d37f940619b432a3a854e23.png)

最后，让我们讨论一下**深度自编码器**。由于以下几个原因，它们一直被认为是进行非线性降维的一个非常好的方法：它们提供了双向灵活映射。学习时间在训练案例数量上是线性的（或更好）。最终的编码模型相当紧凑且快速。然而，使用反向传播来优化深度自编码器非常困难。由于初始权重较小，反向传播的梯度会消失。我们现在有更好的优化方法；可以使用无监督的逐层预训练，或者像在Echo-State Nets中那样小心初始化权重。

对于预训练任务，实际上有3种不同类型的浅层自编码器：

1.  **RBM作为自编码器**：当我们用一步对比散度训练RBM时，它试图使重建看起来像数据。它像自编码器，但通过使用隐藏层中的二进制活动进行强正则化。当用最大似然训练时，RBM不像自编码器。我们可以用一堆浅层自编码器替代用于预训练的RBM堆叠；然而，如果浅层自编码器通过惩罚平方权重进行正则化，则预训练的效果（对于后续的辨别）不如前者。

1.  **去噪自编码器**：这些通过将输入向量的许多组件设置为0（类似于dropout，但用于输入）来添加噪声。它们仍需重建这些组件，因此必须提取能够捕捉输入之间相关性的特征。如果我们使用一系列去噪自编码器，预训练效果非常好。它的效果与使用RBM进行预训练一样好，甚至更好。评估预训练也更简单，因为我们可以轻松计算目标函数的值。它缺乏RBM所提供的优美变分界限，但这仅仅是理论上的兴趣。

1.  **收缩自编码器**：另一种对自编码器进行正则化的方法是尽可能使隐藏单元的活动对输入不敏感；但它们不能忽视输入，因为它们必须重建输入。我们通过惩罚每个隐藏活动相对于输入的平方梯度来实现这一点。收缩自编码器在预训练中效果非常好。编码往往具有这样的特性：只有一小部分隐藏单元对输入的变化敏感。

![](../Images/7ac6f888cc07b24fb8fe9dd7111e91ee.png)

-   简而言之，现在有许多不同的方法可以进行逐层特征预训练。对于没有大量标记案例的数据集，预训练有助于后续的判别学习。对于非常大、标记的数据集，通过无监督预训练初始化用于监督学习的权重并非必要，即使是对于深度网络。预训练曾是初始化深度网络权重的首选方法，但现在有其他方法。不过，如果我们将网络做得更大，我们将需要再次进行预训练！

**最后的收获**

-   神经网络是有史以来最美妙的编程范式之一。在传统编程方法中，我们告诉计算机做什么，将大问题拆分成许多小的、明确定义的任务，计算机可以轻松完成。相比之下，在神经网络中，我们不告诉计算机如何解决问题。相反，它从观察数据中学习，找出自己解决当前问题的方法。

-   今天，深度神经网络和深度学习在计算机视觉、语音识别和自然语言处理等许多重要问题上表现出色。它们正在被谷歌、微软和 Facebook 等公司大规模部署。

-   我希望这篇文章能帮助你学习神经网络的核心概念，包括深度学习的现代技术。你可以从[我的 GitHub 仓库](https://github.com/khanhnamle1994/neural-nets)获取我为 Dr. Hinton 的 Coursera 课程所做的所有讲义、研究论文和编程作业。祝你学习顺利！

**简介：[James Le](https://www.linkedin.com/in/khanhnamle94/)** 目前正在申请美国的计算机科学硕士项目，计划于2018年秋季入学。他的研究方向将集中在机器学习和数据挖掘方面。与此同时，他还在担任自由职业的全栈网页开发人员。

[原文](https://towardsdatascience.com/the-8-neural-network-architectures-machine-learning-researchers-need-to-learn-11a0c96d6073)。经授权转载。

**相关：**

+   [AI 从业者需要应用的10种深度学习方法](/2017/12/10-deep-learning-methods-ai-practitioners-need-apply.html)

+   [机器学习工程师需要了解的10种算法](/2016/08/10-algorithms-machine-learning-engineers.html)

+   [数据科学家需要掌握的10种统计技术](/2017/11/10-statistical-techniques-data-scientists-need-master.html)

* * *

## -   我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析水平

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织在IT方面

* * *

### 了解更多相关话题

+   [成为优秀数据科学家所需的5项关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)

+   [每个初学者数据科学家应该掌握的6种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)

+   [2021年最佳ETL工具](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)

+   [停止学习数据科学以寻找目标，寻找目标去…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)

+   [学习数据科学统计学的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)

+   [成功数据科学家的5个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)
图像块以获取更多数据，并使用图像的左右反射。在测试时，结合来自10个不同图像块的意见：四个224 x 224的角落图像块加上中央的224 x 224图像块，以及这5个图像块的反射。

1.  使用“dropout”对全连接层的权重进行正则化（这些层包含大部分参数）。Dropout意味着在每个训练样本中，随机去除一半的隐藏单元。这防止了隐藏单元过度依赖其他隐藏单元。

![](../Images/adcd19bf34e1806f4d14c72e8c826e70.png)

在硬件要求方面，Alex使用了非常高效的卷积网络实现，运行在2块Nvidia GTX 580 GPU上（超过1000个快速的小核心）。这些GPU非常适合矩阵乘法，并且具有非常高的内存带宽。这使得他能够在一周内训练网络，并且在测试时快速结合来自10个图像块的结果。如果我们能快速地通信状态，我们可以将网络分布到许多核心上。随着核心变得便宜，数据集变得更大，大型神经网络将比传统计算机视觉系统改进得更快。

### 3 — 递归神经网络

![](../Images/165765dffd0351593bc95483a6eccae1.png)

要理解 RNNs，我们需要对序列建模有一个简要的概述。在将机器学习应用于序列时，我们通常希望将输入序列转换为位于不同领域的输出序列；例如，将一系列声音压力转换为一系列单词身份。当没有单独的目标序列时，我们可以通过尝试预测输入序列中的下一个项来获得教学信号。目标输出序列是输入序列的前进 1 步。这似乎比尝试从其他像素预测图像中的一个像素，或从图像的其余部分预测图像中的一个补丁自然得多。预测序列中的下一个项模糊了监督学习和无监督学习之间的区别。它使用了为监督学习设计的方法，但不需要单独的教学信号。

**无记忆模型** 是处理这一任务的标准方法。特别是，自回归模型可以从固定数量的前一个项中预测序列中的下一个项，使用“延迟抽头”；前馈神经网络是广义的自回归模型，使用一个或多个层的非线性隐藏单元。然而，如果我们给生成模型一些隐藏状态，并且如果我们赋予这个隐藏状态自己的内部动态，我们就会得到一种更有趣的模型：它可以在其隐藏状态中存储信息很长时间。如果这些动态是嘈杂的，并且它们从隐藏状态生成输出的方式也是嘈杂的，我们永远无法知道其确切的隐藏状态。我们能做的最好的是推断隐藏状态向量空间上的概率分布。这种推断仅对两种类型的隐藏状态模型是可行的。

**递归神经网络** 非常强大，因为它们结合了两个特性：1) 分布式隐藏状态，使它们能够高效地存储大量关于过去的信息，以及 2) 非线性动态，使它们能够以复杂的方式更新隐藏状态。只要有足够的神经元和时间，RNNs 可以计算出任何计算机能计算的东西。那么 RNNs 可以表现出什么样的行为呢？它们可以发生振荡，可以收敛到点吸引子，可以表现得混乱不堪。它们还有可能学习实现许多小程序，每个程序捕捉一个知识点并并行运行，相互作用产生非常复杂的效果。

![](../Images/e6043e3e88d5c61a2b88f6a281218d60.png)

然而，RNN的计算能力使得它们非常难以训练。由于梯度爆炸或消失问题，训练RNN相当困难。当我们通过多层进行反向传播时，梯度的幅度会发生什么？如果权重很小，梯度会指数级缩小。如果权重很大，梯度会指数级增长。典型的前馈神经网络可以应对这些指数效应，因为它们只有少量隐藏层。另一方面，在训练长序列的RNN中，梯度很容易爆炸或消失。即使初始权重良好，也很难检测到当前目标输出依赖于许多时间步之前的输入，因此RNN在处理长距离依赖时会遇到困难。

学习RNN的有效方法有4种：

+   **长短期记忆**：将RNN构建为旨在长期记忆的模块。

+   **Hessian Free Optimization**：通过使用一种可以检测到微小梯度但曲率更小的复杂优化器来处理梯度消失问题。

+   **回声状态网络**：非常仔细地初始化输入->隐藏层和隐藏层->隐藏层以及输出->隐藏层的连接，使得隐藏状态拥有一个巨大的、弱耦合的振荡器储备，这些振荡器可以通过输入选择性地驱动。

+   **良好的动量初始化**：像在回声状态网络中那样初始化，然后使用动量学习所有的连接。

### 4 — 长短期记忆网络

![](../Images/d6dbe07ea1caf153d300783588d959d4.png)

[Hochreiter & Schmidhuber (1997)](http://www.bioinf.jku.at/publications/older/2604.pdf) 通过建立被称为**长短期记忆网络**的模型解决了使RNN长期记忆的难题（如数百个时间步骤）。他们设计了一个使用逻辑单元和线性单元以及乘法交互的记忆单元。当“写入”门开启时，信息会进入单元。信息会在“保持”门开启时留在单元中。通过打开“读取”门，可以从单元中读取信息。

阅读草书是一项RNN的自然任务。输入是笔尖的（x，y，p）坐标序列，其中p表示笔是上还是下。输出是字符序列。[Graves & Schmidhuber (2009)](http://people.idsia.ch/~juergen/nips2009.pdf) 显示带有LSTM的RNN目前是阅读草书的最佳系统。简言之，他们使用了一系列小图像作为输入，而不是笔坐标。

![](../Images/0de05190c7dbf55ac81c791d617a527e.png)

### 更多相关内容

+   [成为优秀数据科学家所需的5项关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)

+   [每个初学者数据科学家应该掌握的6种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)

+   [2021年最佳ETL工具](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)

+   [停止学习数据科学以寻找目标，并找到目标以…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)

+   [数据科学学习统计的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)

+   [成功数据科学家的5个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)
