- en: How Retrieval Augment Generation Makes LLMs Smarter
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•æ£€ç´¢å¢å¼ºç”Ÿæˆä½¿LLMå˜å¾—æ›´èªæ˜
- en: åŸæ–‡ï¼š[https://www.kdnuggets.com/how-retrieval-augment-generation-makes-llms-smarter](https://www.kdnuggets.com/how-retrieval-augment-generation-makes-llms-smarter)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://www.kdnuggets.com/how-retrieval-augment-generation-makes-llms-smarter](https://www.kdnuggets.com/how-retrieval-augment-generation-makes-llms-smarter)
- en: '![How Retrieval Augment Generation Makes LLMs Smarter Than Before](../Images/ab38ea2eabccfbf98ef8e8a9dc26f39b.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![å¦‚ä½•æ£€ç´¢å¢å¼ºç”Ÿæˆä½¿LLMå˜å¾—æ¯”ä»¥å¾€æ›´èªæ˜](../Images/ab38ea2eabccfbf98ef8e8a9dc26f39b.png)'
- en: Ideal Generative AI vs. Reality
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç†æƒ³ç”Ÿæˆå‹AI vs. ç°å®
- en: Foundational LLMs have read every byte of text they could find and their chatbot
    counterparts can be prompted to have intelligent conversations and be asked to
    perform specific tasks. Access to comprehensive information is democratized; No
    more figuring out the right keywords to search or picking sites to read from.
    However, LLMs are prone to rambling and generally respond with the statistically
    most probable response youâ€™d want to hear ([sycophancy](https://arxiv.org/abs/2310.13548))
    an inherent result of the transformer model. Extracting 100% accurate information
    out of an LLMâ€™s knowledge base doesnâ€™t always yield trustworthy results.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºç¡€çš„LLMï¼ˆå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼‰å·²ç»é˜…è¯»äº†å®ƒä»¬èƒ½æ‰¾åˆ°çš„æ¯ä¸€ä¸ªå­—èŠ‚çš„æ–‡æœ¬ï¼Œå®ƒä»¬çš„èŠå¤©æœºå™¨äººå¯¹ç­‰ä½“å¯ä»¥è¢«æç¤ºè¿›è¡Œæ™ºèƒ½å¯¹è¯ï¼Œå¹¶è¢«è¦æ±‚æ‰§è¡Œç‰¹å®šä»»åŠ¡ã€‚è·å–å…¨é¢çš„ä¿¡æ¯å·²ç»å®ç°äº†æ°‘ä¸»åŒ–ï¼›ä¸å†éœ€è¦æ‰¾å‡ºæ­£ç¡®çš„æœç´¢å…³é”®è¯æˆ–æŒ‘é€‰é˜…è¯»çš„ç½‘ç«™ã€‚ç„¶è€Œï¼ŒLLMå®¹æ˜“å” å¨ï¼Œå¹¶ä¸”é€šå¸¸ä¼šä»¥ç»Ÿè®¡ä¸Šæœ€å¯èƒ½çš„å›åº”æ¥å›ç­”ä½ æƒ³å¬çš„å†…å®¹ï¼ˆ[è°„åªš](https://arxiv.org/abs/2310.13548)ï¼‰ï¼Œè¿™æ˜¯å˜å‹å™¨æ¨¡å‹çš„å›ºæœ‰ç»“æœã€‚ä»LLMçš„çŸ¥è¯†åº“ä¸­æå–100%å‡†ç¡®çš„ä¿¡æ¯å¹¶ä¸æ€»æ˜¯èƒ½å¾—åˆ°å¯ä¿¡çš„ç»“æœã€‚
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„å‰ä¸‰å¤§è¯¾ç¨‹æ¨è
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [è°·æ­Œç½‘ç»œå®‰å…¨è¯ä¹¦](https://www.kdnuggets.com/google-cybersecurity)
    - å¿«é€Ÿè¿›å…¥ç½‘ç»œå®‰å…¨èŒä¸šç”Ÿæ¶¯ã€‚'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [è°·æ­Œæ•°æ®åˆ†æä¸“ä¸šè¯ä¹¦](https://www.kdnuggets.com/google-data-analytics)
    - æå‡ä½ çš„æ•°æ®åˆ†ææŠ€èƒ½'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [è°·æ­ŒITæ”¯æŒä¸“ä¸šè¯ä¹¦](https://www.kdnuggets.com/google-itsupport)
    - æ”¯æŒä½ çš„ç»„ç»‡è¿›è¡ŒITç®¡ç†'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Chat LLMs are infamous for making up citations to scientific papers or court
    cases that donâ€™t exist.Â [Lawyers filing a suit against an airlineÂ ](https://www.theguardian.com/technology/2023/jun/23/two-us-lawyers-fined-submitting-fake-court-citations-chatgpt)included
    citations to court cases that never actually happened.Â [A 2023 study reported](https://arxiv.org/abs/2309.09401),
    that when ChatGPT is prompted to include citations, it had only provided references
    that exist only 14% of the time. Falsifying sources, rambling, and delivering
    inaccuracies to appease the prompt are dubbed hallucination, a huge obstacle to
    overcome before AI is fully adopted and trusted by the masses.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: èŠå¤©å‹LLMå› ç¼–é€ ä¸å­˜åœ¨çš„ç§‘å­¦è®ºæ–‡æˆ–æ³•é™¢æ¡ˆä»¶çš„å¼•ç”¨è€Œè‡­åæ˜­è‘—ã€‚[å¯¹èˆªç©ºå…¬å¸æèµ·è¯‰è®¼çš„å¾‹å¸ˆ](https://www.theguardian.com/technology/2023/jun/23/two-us-lawyers-fined-submitting-fake-court-citations-chatgpt)åŒ…æ‹¬äº†å®é™…ä¸Šä»æœªå‘ç”Ÿè¿‡çš„æ³•é™¢æ¡ˆä»¶çš„å¼•ç”¨ã€‚[2023å¹´çš„ä¸€é¡¹ç ”ç©¶æŠ¥å‘Š](https://arxiv.org/abs/2309.09401)æ˜¾ç¤ºï¼Œå½“ChatGPTè¢«æç¤ºåŒ…å«å¼•ç”¨æ—¶ï¼Œå®ƒä»…åœ¨14%çš„æ—¶é—´é‡Œæä¾›äº†å®é™…å­˜åœ¨çš„å‚è€ƒæ–‡çŒ®ã€‚ä¼ªé€ æ¥æºã€å” å¨ä»¥åŠä¸ºäº†è¿åˆæç¤ºè€Œæä¾›ä¸å‡†ç¡®çš„ä¿¡æ¯è¢«ç§°ä¸ºå¹»è§‰ï¼Œè¿™æ˜¯åœ¨AIè¢«å¤§ä¼—å…¨é¢é‡‡çº³å’Œä¿¡ä»»ä¹‹å‰éœ€è¦å…‹æœçš„ä¸€å¤§éšœç¢ã€‚
- en: One counter to LLMs making up bogus sources or coming up with inaccuracies is
    retrieval-augmented generation or RAG. Not only can RAG decrease the tendency
    of LLMs to hallucinate but several other advantages as well.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åº”å¯¹LLMç¼–é€ è™šå‡æ¥æºæˆ–äº§ç”Ÿä¸å‡†ç¡®å†…å®¹çš„ä¸€ç§å¯¹ç­–æ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€‚RAGä¸ä»…å¯ä»¥å‡å°‘LLMçš„å¹»è§‰å€¾å‘ï¼Œè¿˜æœ‰å…¶ä»–å‡ ä¸ªä¼˜åŠ¿ã€‚
- en: These advantages include access to an updated knowledge base, specialization
    (*e.g.*Â by providing private data sources), empowering models with information
    beyond what is stored in the parametric memory (allowing for smaller models),
    and the potential to follow up with more data from legitimate references.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ä¼˜åŠ¿åŒ…æ‹¬è®¿é—®æ›´æ–°çš„çŸ¥è¯†åº“ï¼Œä¸“ä¸šåŒ–ï¼ˆ*ä¾‹å¦‚* æä¾›ç§æœ‰æ•°æ®æºï¼‰ï¼Œèµ‹äºˆæ¨¡å‹è¶…è¶Šå‚æ•°è®°å¿†ä¸­å­˜å‚¨çš„ä¿¡æ¯ï¼ˆå…è®¸æ›´å°çš„æ¨¡å‹ï¼‰ï¼Œä»¥åŠæœ‰å¯èƒ½ä»åˆæ³•çš„å‚è€ƒèµ„æ–™ä¸­è·å¾—æ›´å¤šçš„æ•°æ®ã€‚
- en: What is RAG (Retrieval Augmented Generation)?
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ï¼Ÿ
- en: 'Retrieval-Augmented Generation (RAG) is a deep learning architecture implemented
    in LLMs and transformer networks that retrieves relevant documents or other snippets
    and adds them to the context window to provide additional information, aiding
    an LLM to generate useful responses. A typical RAG system would have two main
    modules: retrieval and generation.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯ä¸€ç§æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œå®æ–½åœ¨LLMså’Œå˜æ¢å™¨ç½‘ç»œä¸­ï¼Œé€šè¿‡æ£€ç´¢ç›¸å…³æ–‡æ¡£æˆ–å…¶ä»–ç‰‡æ®µå¹¶å°†å…¶æ·»åŠ åˆ°ä¸Šä¸‹æ–‡çª—å£ä¸­ä»¥æä¾›é¢å¤–çš„ä¿¡æ¯ï¼Œä»è€Œå¸®åŠ©LLMç”Ÿæˆæœ‰ç”¨çš„å“åº”ã€‚ä¸€ä¸ªå…¸å‹çš„RAGç³»ç»Ÿæœ‰ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ£€ç´¢å’Œç”Ÿæˆã€‚
- en: '![retrieval augmented generation architecture - RAG](../Images/d69cffca46379cd698dcd2203c816120.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![æ£€ç´¢å¢å¼ºç”Ÿæˆæ¶æ„ - RAG](../Images/d69cffca46379cd698dcd2203c816120.png)'
- en: The main reference for RAG is aÂ [paper by Lewis et al.](https://arxiv.org/abs/2005.11401)Â from
    Facebook. In the paper, the authors use a pair of BERT-based document encoders
    to transform queries and documents by embedding the text in a vector format. These
    embeddings are then used to identify the top-*k*Â (typically 5 or 10) documents
    via a maximum inner product search (MIPS). As the name suggests, MIPS is based
    on the inner (or dot) product of the encoded vector representations of the query
    and those in a vector database pre-computed for the documents used as external,
    non-parametric memory.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: RAGçš„ä¸»è¦å‚è€ƒæ–‡çŒ®æ˜¯Facebookçš„[ä¸€ç¯‡ç”±Lewisç­‰äººæ’°å†™çš„è®ºæ–‡](https://arxiv.org/abs/2005.11401)ã€‚åœ¨è®ºæ–‡ä¸­ï¼Œä½œè€…ä½¿ç”¨ä¸€å¯¹åŸºäºBERTçš„æ–‡æ¡£ç¼–ç å™¨é€šè¿‡å°†æ–‡æœ¬åµŒå…¥åˆ°å‘é‡æ ¼å¼ä¸­æ¥è½¬æ¢æŸ¥è¯¢å’Œæ–‡æ¡£ã€‚è¿™äº›åµŒå…¥éšåç”¨äºé€šè¿‡æœ€å¤§å†…ç§¯æœç´¢ï¼ˆMIPSï¼‰æ¥è¯†åˆ«å‰*k*ï¼ˆé€šå¸¸æ˜¯5æˆ–10ï¼‰ä¸ªæ–‡æ¡£ã€‚é¡¾åæ€ä¹‰ï¼ŒMIPSåŸºäºæŸ¥è¯¢çš„ç¼–ç å‘é‡è¡¨ç¤ºä¸é¢„è®¡ç®—çš„æ–‡æ¡£å‘é‡æ•°æ®åº“ä¸­çš„å‘é‡è¡¨ç¤ºä¹‹é—´çš„å†…ç§¯ï¼ˆæˆ–ç‚¹ç§¯ï¼‰ã€‚
- en: As described in the piece by LewisÂ *et al.*, RAG was designed to make LLMs better
    at knowledge-intensive tasks which â€œhumans could not reasonably be expected to
    perform without access to an external knowledge sourceâ€. Consider taking an open
    book and non-open book exam and youâ€™ll have a good indication of how RAG might
    supplement LLM-based systems.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚Lewis *ç­‰äºº* åœ¨æ–‡ç« ä¸­æ‰€æè¿°çš„é‚£æ ·ï¼ŒRAGçš„è®¾è®¡ç›®çš„æ˜¯è®©LLMsåœ¨å¤„ç†â€œäººç±»åœ¨æ²¡æœ‰å¤–éƒ¨çŸ¥è¯†æ¥æºçš„æƒ…å†µä¸‹æ— æ³•åˆç†å®Œæˆçš„çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡â€æ—¶è¡¨ç°å¾—æ›´å¥½ã€‚è€ƒè™‘ä¸€ä¸‹å¼€æ”¾ä¹¦ç±å’Œéå¼€æ”¾ä¹¦ç±è€ƒè¯•çš„å¯¹æ¯”ï¼Œä½ å°±èƒ½å¾ˆå¥½åœ°ç†è§£RAGå¦‚ä½•è¡¥å……åŸºäºLLMçš„ç³»ç»Ÿã€‚
- en: RAG with the Hugging Face ğŸ¤— Library
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Hugging Face ğŸ¤— åº“çš„RAG
- en: LewisÂ *et al.*Â open-sourced their RAG models on the Hugging Face Hub, thus we
    can experiment with the same models used in the paper. A new Python 3.8 virtual
    environment with virtualenv is recommended.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Lewis *ç­‰äºº* åœ¨Hugging Face Hubä¸Šå¼€æºäº†ä»–ä»¬çš„RAGæ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è®ºæ–‡ä¸­ç›¸åŒçš„æ¨¡å‹è¿›è¡Œå®éªŒã€‚æ¨èä½¿ç”¨Python 3.8çš„è™šæ‹Ÿç¯å¢ƒï¼Œå¹¶ä½¿ç”¨virtualenvã€‚
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After activating the environment, we can install dependencies using pip: transformers
    and datasets from Hugging Face, the FAISS library from Facebook that RAG uses
    for vector search, and PyTorch for use as a backend.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ¿€æ´»ç¯å¢ƒåï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨pipå®‰è£…ä¾èµ–é¡¹ï¼šæ¥è‡ªHugging Faceçš„transformerså’Œdatasetsï¼ŒRAGä½¿ç”¨çš„Facebookçš„FAISSåº“ç”¨äºå‘é‡æœç´¢ï¼Œä»¥åŠç”¨äºä½œä¸ºåç«¯çš„PyTorchã€‚
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'LewisÂ *et al.*Â implemented two different versions of RAG: rag-sequence and
    rag-token. Rag-sequence uses the same retrieved document to augment the generation
    of an entire sequence whereas rag-token can use different snippets for each token.
    Both versions use the same Hugging Face classes for tokenization and retrieval,
    and the API is much the same, but each version has a unique class for generation.
    These classes are imported from the transformers library.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Lewis *ç­‰äºº* å®ç°äº†ä¸¤ç§ä¸åŒç‰ˆæœ¬çš„RAGï¼šrag-sequence å’Œ rag-tokenã€‚rag-sequenceä½¿ç”¨ç›¸åŒçš„æ£€ç´¢æ–‡æ¡£æ¥å¢å¼ºæ•´ä¸ªåºåˆ—çš„ç”Ÿæˆï¼Œè€Œrag-tokenå¯ä»¥ä¸ºæ¯ä¸ªæ ‡è®°ä½¿ç”¨ä¸åŒçš„ç‰‡æ®µã€‚è¿™ä¸¤ä¸ªç‰ˆæœ¬éƒ½ä½¿ç”¨ç›¸åŒçš„Hugging
    Faceç±»è¿›è¡Œæ ‡è®°åŒ–å’Œæ£€ç´¢ï¼ŒAPIä¹ŸåŸºæœ¬ç›¸åŒï¼Œä½†æ¯ä¸ªç‰ˆæœ¬éƒ½æœ‰ä¸€ä¸ªç‹¬ç‰¹çš„ç”Ÿæˆç±»ã€‚è¿™äº›ç±»æ˜¯ä»transformersåº“ä¸­å¯¼å…¥çš„ã€‚
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The first time the RagRetriever model with the default â€œwiki_dprâ€ dataset is
    instantiated it will initiate a substantial download (about 300 GB). If you have
    a large data drive and want Hugging Face to use it (instead of the default cache
    folder in your home drive), you can set a shell variable, HF_DATASETS_CACHE.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å½“é¦–æ¬¡å®ä¾‹åŒ–å…·æœ‰é»˜è®¤â€œwiki_dprâ€æ•°æ®é›†çš„RagRetrieveræ¨¡å‹æ—¶ï¼Œå®ƒå°†å¯åŠ¨å¤§è§„æ¨¡ä¸‹è½½ï¼ˆçº¦300 GBï¼‰ã€‚å¦‚æœä½ æœ‰ä¸€ä¸ªå¤§å‹æ•°æ®é©±åŠ¨å™¨å¹¶å¸Œæœ›Hugging
    Faceä½¿ç”¨å®ƒï¼ˆè€Œä¸æ˜¯é»˜è®¤çš„ç¼“å­˜æ–‡ä»¶å¤¹ï¼‰ï¼Œå¯ä»¥è®¾ç½®ä¸€ä¸ªShellå˜é‡HF_DATASETS_CACHEã€‚
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Ensure the code is working before downloading the full wiki_dpr dataset. To
    avoid the big download until youâ€™re ready, you can pass use_dummy_dataset=True
    when instantiating the retriever. Youâ€™ll also instantiate a tokenizer to convert
    strings to integer indices (corresponding to tokens in a vocabulary) and vice-versa.
    Sequence and token versions of RAG use the same tokenizer. RAG sequence (rag-sequence)
    and RAG token (rag-token) each have fine-tuned (*e.g.Â *rag-token-nq) and base
    versions (*e.g.*Â rag-token-base).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹è½½å®Œæ•´çš„wiki_dpræ•°æ®é›†ä¹‹å‰ï¼Œè¯·ç¡®ä¿ä»£ç æ­£å¸¸è¿è¡Œã€‚ä¸ºäº†é¿å…å¤§è§„æ¨¡ä¸‹è½½ç›´åˆ°ä½ å‡†å¤‡å¥½ï¼Œä½ å¯ä»¥åœ¨å®ä¾‹åŒ–æ£€ç´¢å™¨æ—¶ä¼ é€’use_dummy_dataset=Trueã€‚ä½ è¿˜éœ€è¦å®ä¾‹åŒ–ä¸€ä¸ªæ ‡è®°å™¨ï¼Œå°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°ç´¢å¼•ï¼ˆå¯¹åº”äºè¯æ±‡è¡¨ä¸­çš„æ ‡è®°ï¼‰åŠåä¹‹ã€‚RAGçš„åºåˆ—ç‰ˆæœ¬å’Œæ ‡è®°ç‰ˆæœ¬ä½¿ç”¨ç›¸åŒçš„æ ‡è®°å™¨ã€‚RAGåºåˆ—ï¼ˆrag-sequenceï¼‰å’ŒRAGæ ‡è®°ï¼ˆrag-tokenï¼‰å„è‡ªæœ‰ç»è¿‡å¾®è°ƒçš„ï¼ˆ*ä¾‹å¦‚*
    rag-token-nqï¼‰å’ŒåŸºç¡€ç‰ˆæœ¬ï¼ˆ*ä¾‹å¦‚* rag-token-baseï¼‰ã€‚
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Once your models are instantiated, you can provide a query, tokenize it, and
    pass it to the â€œgenerateâ€ function of the model. Weâ€™ll compare results from rag-sequence,
    rag-token, and RAG using a retriever with the dummy version of the wiki_dpr dataset.Â Note
    that these rag-models are case-insensitive
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦ä½ çš„æ¨¡å‹è¢«å®ä¾‹åŒ–ï¼Œä½ å¯ä»¥æä¾›æŸ¥è¯¢ï¼Œè¿›è¡Œæ ‡è®°åŒ–ï¼Œç„¶åå°†å…¶ä¼ é€’ç»™æ¨¡å‹çš„â€œgenerateâ€å‡½æ•°ã€‚æˆ‘ä»¬å°†æ¯”è¾ƒä½¿ç”¨å¸¦æœ‰dummyç‰ˆæœ¬wiki_dpræ•°æ®é›†çš„rag-sequenceã€rag-tokenå’ŒRAGçš„ç»“æœã€‚è¯·æ³¨æ„ï¼Œè¿™äº›ragæ¨¡å‹æ˜¯ä¸åŒºåˆ†å¤§å°å†™çš„ã€‚
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In general, rag-token is correct more often than rag-sequence, (though both
    are often correct), and rag-sequence is more often right than RAG using a retriever
    with a dummy dataset.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œrag-tokençš„æ­£ç¡®ç‡é«˜äºrag-sequenceï¼ˆå°½ç®¡ä¸¤è€…éƒ½ç»å¸¸æ­£ç¡®ï¼‰ï¼Œè€Œrag-sequenceçš„æ­£ç¡®ç‡é«˜äºä½¿ç”¨dummyæ•°æ®é›†çš„RAGã€‚
- en: â€œWhat sort of context does the retriever provide?â€ You may wonder. To find out,
    we can deconstruct the generation process. Using the seq_retriever and seq_model
    instantiated as above, we query â€œWhat is the name of the oldest tree on Earthâ€
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: â€œæ£€ç´¢å™¨æä¾›äº†ä»€ä¹ˆæ ·çš„ä¸Šä¸‹æ–‡ï¼Ÿâ€ä½ å¯èƒ½ä¼šå¥½å¥‡ã€‚ä¸ºäº†æ‰¾å‡ºç­”æ¡ˆï¼Œæˆ‘ä»¬å¯ä»¥è§£æ„ç”Ÿæˆè¿‡ç¨‹ã€‚ä½¿ç”¨ä¸Šé¢å®ä¾‹åŒ–çš„seq_retrieverå’Œseq_modelï¼Œæˆ‘ä»¬æŸ¥è¯¢â€œåœ°çƒä¸Šæœ€å¤è€çš„æ ‘å«ä»€ä¹ˆåå­—â€
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can code our model to print the variable â€œbest contextâ€ to see what was captured
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç¼–å†™ä»£ç è®©æ¨¡å‹æ‰“å°â€œbest contextâ€å˜é‡ï¼Œä»¥æŸ¥çœ‹æ•è·äº†ä»€ä¹ˆã€‚
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'based on the retrieved context:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼š
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can also print the answer by calling the `generated_string` variable. The
    rag-sequence-nq answers 'what is the name of the oldest tree on Earth?' with 'Prometheus'.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡è°ƒç”¨`generated_string`å˜é‡æ¥æ‰“å°ç­”æ¡ˆã€‚rag-sequence-nqå›ç­”â€œåœ°çƒä¸Šæœ€å¤è€çš„æ ‘å«ä»€ä¹ˆåå­—ï¼Ÿâ€ä¸ºâ€œPrometheusâ€ã€‚
- en: What Can You Do with RAG?
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ç”¨RAGåšä»€ä¹ˆï¼Ÿ
- en: In the last year and a half, there has been a veritable explosion in LLMs and
    LLM tools. The BART base model used in LewisÂ *et al.*Â was only 400 million parameters,
    a far cry from the current crop of LLMs, which typically start in the billion
    parameter range for â€œliteâ€ variants. Also, many models being trained, merged,
    and fine-tuned today are multimodal, combining text inputs and outputs with images
    or other tokenized data sources. Combining RAG with other tools can build complex
    capabilities, but the underlying models wonâ€™t be immune to common LLM shortcomings.
    The problems of sycophancy, hallucination, and reliability in LLMs all remain
    and run the risk of growing just as LLM use grows.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿‡å»ä¸€å¹´åŠé‡Œï¼ŒLLMså’ŒLLMå·¥å…·ç»å†äº†çœŸæ­£çš„çˆ†ç‚¸æ€§å¢é•¿ã€‚Lewis *ç­‰* ä½¿ç”¨çš„BARTåŸºç¡€æ¨¡å‹ä»…æœ‰4äº¿ä¸ªå‚æ•°ï¼Œä¸å½“å‰çš„LLMsç›¸æ¯”ï¼Œåè€…é€šå¸¸ä»¥åäº¿å‚æ•°çº§åˆ«çš„â€œè½»é‡â€å˜ä½“ä¸ºèµ·ç‚¹ã€‚æ­¤å¤–ï¼Œè®¸å¤šæ­£åœ¨è®­ç»ƒã€åˆå¹¶å’Œå¾®è°ƒçš„æ¨¡å‹éƒ½æ˜¯å¤šæ¨¡æ€çš„ï¼Œå°†æ–‡æœ¬è¾“å…¥å’Œè¾“å‡ºä¸å›¾åƒæˆ–å…¶ä»–æ ‡è®°åŒ–çš„æ•°æ®æºç»“åˆèµ·æ¥ã€‚å°†RAGä¸å…¶ä»–å·¥å…·ç»“åˆå¯ä»¥æ„å»ºå¤æ‚çš„èƒ½åŠ›ï¼Œä½†åŸºç¡€æ¨¡å‹ä»ç„¶ä¸èƒ½å…äºå¸¸è§LLMç¼ºé™·ã€‚è°„åªšã€å¹»è§‰å’Œå¯é æ€§çš„é—®é¢˜ä¾ç„¶å­˜åœ¨ï¼Œå¹¶æœ‰å¯èƒ½éšç€LLMçš„ä½¿ç”¨å¢é•¿è€ŒåŠ å‰§ã€‚
- en: The most obvious applications for RAG are variations on conversational semantic
    search, but perhaps they also include incorporating multimodal inputs or image
    generation as part of the output. For example, RAG in LLMs with domain knowledge
    can make software documentation you can chat with. Or RAGÂ could be usedÂ to keep
    interactive notes in a literature review for a research project or thesis.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: RAGæœ€æ˜æ˜¾çš„åº”ç”¨æ˜¯å¯¹è¯è¯­ä¹‰æœç´¢çš„å˜ä½“ï¼Œä½†å®ƒä»¬ä¹Ÿå¯èƒ½åŒ…æ‹¬å°†å¤šæ¨¡æ€è¾“å…¥æˆ–å›¾åƒç”Ÿæˆä½œä¸ºè¾“å‡ºçš„ä¸€éƒ¨åˆ†ã€‚ä¾‹å¦‚ï¼Œå…·æœ‰é¢†åŸŸçŸ¥è¯†çš„LLMä¸­çš„RAGå¯ä»¥ç”Ÿæˆä½ å¯ä»¥èŠå¤©çš„è½¯ä»¶æ–‡æ¡£ã€‚æˆ–è€…RAGå¯ä»¥ç”¨äºåœ¨æ–‡çŒ®ç»¼è¿°çš„ç ”ç©¶é¡¹ç›®æˆ–è®ºæ–‡ä¸­ä¿æŒäº’åŠ¨ç¬”è®°ã€‚
- en: Incorporating a â€˜chain-of-thoughtâ€™ reasoning capability, you could take a more
    agentic approach to empower your models to query RAG system and assemble more
    complex lines of inquiry or reasoning.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æ•´åˆâ€œé“¾å¼æ€ç»´â€æ¨ç†èƒ½åŠ›ï¼Œä½ å¯ä»¥é‡‡å–æ›´å…·ä»£ç†æ€§çš„æ–¹å¼ï¼Œèµ‹èƒ½ä½ çš„æ¨¡å‹æŸ¥è¯¢RAGç³»ç»Ÿå¹¶ç»„è£…æ›´å¤æ‚çš„è¯¢é—®æˆ–æ¨ç†è·¯å¾„ã€‚
- en: It is also very important to keep in mind that RAG does not solve the common
    LLM pitfalls (hallucination, sycophancy, etc.) and serves only as a means to alleviate
    or guide your LLM to a more niche response. The endpoints that ultimately matter,
    are specific to your use case, the information you feed your model, and how the
    model is finetuned.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜éœ€è¦ç‰¹åˆ«æ³¨æ„çš„æ˜¯ï¼ŒRAG å¹¶ä¸èƒ½è§£å†³å¸¸è§çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é—®é¢˜ï¼ˆå¦‚å¹»è§‰ã€è°„åªšç­‰ï¼‰ï¼Œå®ƒä»…ä½œä¸ºä¸€ç§ç¼“è§£æˆ–å¼•å¯¼ä½ çš„ LLM è¾¾åˆ°æ›´ä¸“ä¸šå›åº”çš„æ‰‹æ®µã€‚æœ€ç»ˆé‡è¦çš„æ˜¯ï¼Œå–å†³äºä½ çš„ä½¿ç”¨æ¡ˆä¾‹ã€ä½ æä¾›ç»™æ¨¡å‹çš„ä¿¡æ¯ä»¥åŠæ¨¡å‹çš„å¾®è°ƒæ–¹å¼ã€‚
- en: '**[Kevin Vu](https://blog.exxactcorp.com/)** manages [Exxact Corp blog](https://blog.exxactcorp.com/)
    and works with many of its talented authors who write about different aspects
    of Deep Learning.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Kevin Vu](https://blog.exxactcorp.com/)** è´Ÿè´£ç®¡ç† [Exxact Corp åšå®¢](https://blog.exxactcorp.com/)ï¼Œå¹¶ä¸è®¸å¤šæ‰åæ¨ªæº¢çš„ä½œè€…åˆä½œï¼Œä»–ä»¬æ’°å†™å…³äºæ·±åº¦å­¦ä¹ ä¸åŒæ–¹é¢çš„æ–‡ç« ã€‚'
- en: More On This Topic
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ›´å¤šç›¸å…³è¯é¢˜
- en: '[Retrieval Augmented Generation: Where Information Retrieval Meetsâ€¦](https://www.kdnuggets.com/retrieval-augmented-generation-where-information-retrieval-meets-text-generation)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ£€ç´¢å¢å¼ºç”Ÿæˆï¼šä¿¡æ¯æ£€ç´¢ä¸â€¦çš„ç»“åˆ](https://www.kdnuggets.com/retrieval-augmented-generation-where-information-retrieval-meets-text-generation)'
- en: '[Making Intelligent Document Processing Smarter: Part 1](https://www.kdnuggets.com/2023/02/making-intelligent-document-processing-smarter-part-1.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è®©æ™ºèƒ½æ–‡æ¡£å¤„ç†æ›´æ™ºèƒ½ï¼šç¬¬ 1 éƒ¨åˆ†](https://www.kdnuggets.com/2023/02/making-intelligent-document-processing-smarter-part-1.html)'
- en: '[How to Optimize SQL Queries for Faster Data Retrieval](https://www.kdnuggets.com/2023/06/optimize-sql-queries-faster-data-retrieval.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å¦‚ä½•ä¼˜åŒ– SQL æŸ¥è¯¢ä»¥åŠ å¿«æ•°æ®æ£€ç´¢é€Ÿåº¦](https://www.kdnuggets.com/2023/06/optimize-sql-queries-faster-data-retrieval.html)'
- en: '[Bark: The Ultimate Audio Generation Model](https://www.kdnuggets.com/2023/05/bark-ultimate-audio-generation-model.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Barkï¼šç»ˆæéŸ³é¢‘ç”Ÿæˆæ¨¡å‹](https://www.kdnuggets.com/2023/05/bark-ultimate-audio-generation-model.html)'
- en: '[The Future of AI: Exploring the Next Generation of Generative Models](https://www.kdnuggets.com/2023/05/future-ai-exploring-next-generation-generative-models.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[äººå·¥æ™ºèƒ½çš„æœªæ¥ï¼šæ¢ç´¢ä¸‹ä¸€ä»£ç”Ÿæˆæ¨¡å‹](https://www.kdnuggets.com/2023/05/future-ai-exploring-next-generation-generative-models.html)'
- en: '[Unveiling Midjourney 5.2: A Leap Forward in AI Image Generation](https://www.kdnuggets.com/2023/06/unveiling-midjourney-52-leap-forward.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ­ç¤º Midjourney 5.2ï¼šAI å›¾åƒç”Ÿæˆçš„è·ƒè¿›](https://www.kdnuggets.com/2023/06/unveiling-midjourney-52-leap-forward.html)'
