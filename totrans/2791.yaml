- en: Introduction to Kubeflow MPI Operator and Industry Adoption
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/03/introduction-kubeflow-mpi-operator-industry-adoption.html](https://www.kdnuggets.com/2020/03/introduction-kubeflow-mpi-operator-industry-adoption.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Yuan Tang](https://twitter.com/TerryTangYuan) (Ant Financial), [Wei Yan](https://www.linkedin.com/in/wei-yan-a6037337/) (Ant
    Financial), and [Rong Ou](https://www.linkedin.com/in/rongou/) (NVIDIA)**'
  prefs: []
  type: TYPE_NORMAL
- en: Kubeflow just [announced its first major 1.0 release recently](https://medium.com/kubeflow/kubeflow-1-0-cloud-native-ml-for-everyone-a3950202751),
    which makes it easy for machine learning engineers and data scientists to leverage
    cloud assets (public or on-premise) for machine learning workloads. In this post,
    we’d like to introduce [MPI Operator](https://github.com/kubeflow/mpi-operator) ([docs](https://www.kubeflow.org/docs/components/training/mpi/)),
    one of the core components of Kubeflow, currently in alpha, which makes it easy
    to run synchronized, allreduce-style distributed training on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two major distributed training strategies nowadays: one based on
    parameter servers and the other based on collective communication primitives such
    as allreduce.'
  prefs: []
  type: TYPE_NORMAL
- en: Parameter server based distribution strategy relies on centralized parameter
    servers for coordination between workers, responsible for collecting gradients
    from workers and sending updated parameters to workers. The diagram below shows
    the interaction between parameter servers and worker nodes under this distributed
    training strategy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/37e3b9f2bc6a0aa5379e087e837d624a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'While distributed training based on parameter servers can support training
    very large models and datasets by adding more workers and parameter servers, there
    are additional challenges involved in order to optimize the performance:'
  prefs: []
  type: TYPE_NORMAL
- en: It is not easy to identify the right ratio of the number of workers to the number
    of parameter servers. For example, if only a small number of parameter servers
    are used, network communication will likely become the bottleneck for training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If many parameter servers are used, the communication may saturate network interconnects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The memory quota of workers and parameter servers requires fine tuning to avoid
    out-of-memory errors or memory waste.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model could fit within the computational resources of each worker, additional
    maintenance and communication overheads are introduced when the model is partitioned
    to multiple parameter servers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to replicate the model on each parameter server in order to support
    fault-tolerance, which requires additional computational and storage resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In contrast, distributed training based on collective communication primitives
    such as [allreduce](https://mpitutorial.com/tutorials/mpi-reduce-and-allreduce/) could
    be more efficient and easier to use in certain use cases. Under allreduce-based
    distributed training strategy, each worker stores a complete set of model parameters.
    In other words, no parameter server is needed. Allreduce-based distributed training
    could address many of the challenges mentioned above:'
  prefs: []
  type: TYPE_NORMAL
- en: Each worker stores a complete set of model parameters, no parameter server is
    needed, so it’s straightforward to add more workers when necessary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Failures among the workers can be recovered easily by restarting the failed
    workers and then load the current model from any of the existing workers. Model
    does not need to be replicated to support fault-tolerance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model can be updated more efficiently by fully leveraging the network structure
    and collective communication algorithms. For example, in [ring-allreduce algorithm](http://research.baidu.com/bringing-hpc-techniques-deep-learning/),
    each of the N workers only needs to communicate with two of its peer workers 2
    * (N − 1) times to update all the model parameters completely.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling up and down the number of workers is as easy as reconstructing the underlying
    allreduce communicator and re-assigning the ranks among the workers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many existing technologies available that provide implementations
    for these collective communication primitives such as [NCCL](https://github.com/NVIDIA/nccl), [Gloo](https://github.com/facebookincubator/gloo/),
    and many different implementations of [MPI](https://www.mpi-forum.org/).
  prefs: []
  type: TYPE_NORMAL
- en: MPI Operator provides a common [Custom Resource Definition (CRD)](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#customresourcedefinitions) for
    defining a training job on a single CPU/GPU, multiple CPU/GPUs, and multiple nodes.
    It also implements a custom controller to manage the CRD, create dependent resources,
    and reconcile the desired states.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e3fd3a774aea0e0b02f1507bcfa5a09c.png)'
  prefs: []
  type: TYPE_IMG
- en: Unlike other operators in Kubeflow such as [TF Operator](https://github.com/kubeflow/tf-operator) and [PyTorch
    Operator](https://github.com/kubeflow/pytorch-operator) that only supports for
    one machine learning framework, MPI operator is decoupled from underlying framework
    so it can work well with many frameworks such as [Horovod](https://github.com/horovod/horovod/), [TensorFlow](https://www.tensorflow.org/), [PyTorch](https://pytorch.org/), [Apache
    MXNet](https://mxnet.apache.org/), and various collective communication implementations
    such as [OpenMPI](https://www.open-mpi.org/).
  prefs: []
  type: TYPE_NORMAL
- en: For more details on comparisons between different distributed training strategies,
    various Kubeflow operators, please check out [our presentation at KubeCon Europe
    2019](https://kccnceu19.sched.com/event/MPaT).
  prefs: []
  type: TYPE_NORMAL
- en: Example API Spec
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ve been working closely with the community and industry adopters to improve
    the API spec for MPI Operator so it’s suitable for many different use cases. Below
    is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: Note that MPI Operator provides a flexible but user-friendly API that’s consistent
    across other Kubeflow operators.
  prefs: []
  type: TYPE_NORMAL
- en: Users can easily customize their launcher and worker pod specs by modifying
    the relevant sections in the template. For example, customizing to use various
    types of computational resources such as CPUs, GPUs, memory, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, below is an example spec that performs distributed TensorFlow
    training job using ImageNet data in [TFRecords](https://www.tensorflow.org/tutorials/load_data/tfrecord) format
    stored in a [Kubernetes volume](https://kubernetes.io/docs/reference/glossary/?all=true#term-volume):'
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'MPI Operator contains a custom controller that listens for changes in MPIJob
    resources. When a new MPIJob is created, the controller goes through the following *logical* steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a [ConfigMap](https://kubernetes.io/docs/reference/glossary/?all=true#term-configmap) that
    contains:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A helper shell script that can be used by mpirun in place of ssh. It invokes
    kubectl exec for remote execution.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A hostfile that lists the pods in the worker [StatefulSet](https://kubernetes.io/docs/reference/glossary/?all=true#term-statefulset) (in
    the form of ${job-id}-worker-0, ${job-id}-worker-1, …), and the available slots
    (CPUs/GPUs) in each pod.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Create the [RBAC](https://kubernetes.io/docs/reference/glossary/?all=true#term-rbac) resources
    (Role, ServiceAccount, RoleBinding) to allow remote execution (pods/exec).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wait for the worker pods to be ready.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the launcher job. It runs under the [ServiceAccount](https://kubernetes.io/docs/reference/glossary/?all=true#term-service-account) created
    in step 2, and sets up the necessary environment variables for executing mpirun
    commands remotely. The [kubectl](https://kubernetes.io/docs/reference/kubectl/overview/) binary
    is delivered to an emptyDir volume through an init container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the launcher job finishes, set the replicas to 0 in the worker StatefulSet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/7f43db7553c6a09f11cb5760c85cb272.png)'
  prefs: []
  type: TYPE_IMG
- en: For more details, please check out [the design doc for MPI Operator](https://github.com/kubeflow/community/blob/master/proposals/mpi-operator-proposal.md).
  prefs: []
  type: TYPE_NORMAL
- en: Industry Adoption
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At the time of writing, there are [13 disclosed industry adopters](https://github.com/kubeflow/mpi-operator/blob/master/ADOPTERS.md) and
    many others who’ve been working closely with the community to reach where we are
    today. We’d like to showcase some of the use cases of MPI Operator in several
    companies. If your company would like to be included in the list of adopters,
    please send us a pull request [on GitHub](https://github.com/kubeflow/mpi-operator)!
  prefs: []
  type: TYPE_NORMAL
- en: Ant Financial
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At [Ant Financial](https://www.antfin.com/), [we manage Kubernetes clusters
    with tens of thousands of nodes](https://kubernetes.io/case-studies/ant-financial/) and
    have deployed the MPI Operator along with other Kubeflow operators. The MPI Operator
    leverages the network structure and collective communication algorithms so that
    users don’t have to worry about the right ratio between the number of workers
    and parameter servers to obtain the best performance. Users can focus on building
    out their model architectures without spending time on tuning the downstream infrastructure
    for distributed training.
  prefs: []
  type: TYPE_NORMAL
- en: The models produced have been widely deployed in production and battle-tested
    in many different real life scenarios. One notable use case is [Saofu](https://yq.aliyun.com/articles/563095) —
    a mobile app for users to scan any “[福](https://zh.wikipedia.org/wiki/%E7%A6%8F%E5%AD%97)”
    (Chinese character that represents fortune) through augmented reality to enter
    a lucky draw where each user would receive a virtual red envelope with a portion
    of a significant amount of money.
  prefs: []
  type: TYPE_NORMAL
- en: Bloomberg
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Bloomberg](https://www.bloomberg.com/), the global business and financial
    information and news leader, possesses an enormous amount of data — from historical
    news to real-time market data and everything in between. Bloomberg’s Data Science
    Platform was built to allow the company’s internal machine learning engineers
    and data scientists to more easily leverage data and algorithmic models in their
    daily work, including when training jobs and automatic machine learning models
    used in the state-of-the-art solutions they’re building.'
  prefs: []
  type: TYPE_NORMAL
- en: “The Data Science Platform at Bloomberg offers a TensorFlowJob CRD similar to
    Kubeflow’s own TFJob, enabling the company’s data scientists to easily train neural
    network models. Recently, the Data Science Platform team enabled Horovod-based
    distributed training in its TensorFlowJob via the MPI Operator as an implementation
    detail. Using MPIJob in the back-end has allowed the Bloomberg Data Science Platform
    team to quickly offer its machine learning engineers a robust way to train a [BERT
    model](https://arxiv.org/abs/1810.04805) within hours using the company’s large
    corpus of text data’’, says Chengjian Zheng, software engineer from Bloomberg.
  prefs: []
  type: TYPE_NORMAL
- en: Caicloud
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Caicloud Clever](https://intl.caicloud.io/products/clever) is an artificial
    intelligence cloud platform based on Caicloud container cloud platform with powerful
    hardware resource management and efficient model development capabilities. Caicloud
    products have been deployed in many 500 China Fortune companies.'
  prefs: []
  type: TYPE_NORMAL
- en: “Caicloud Clever supports multiple frameworks of AI model training including
    TensorFlow, Apache MXNet, Caffe, PyTorch with the help of Kubeflow tf-operator,
    pytorch-operator and others”, says Ce Gao, AI infrastructure engineer from Caicloud
    Clever team. “While RingAllReduce distributed training support is requested for
    improved customer maturity.”
  prefs: []
  type: TYPE_NORMAL
- en: Kubeflow MPI operator is a Kubernetes Operator for allreduce-style distributed
    training. Caicloud Clever team adopts MPI Operator’s v1alpha2 API. The Kubernetes
    native API makes it easy to work with the existing systems in the platform.
  prefs: []
  type: TYPE_NORMAL
- en: Iguazio
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Iguazio](https://www.iguazio.com/) provides a cloud-native data science platform
    with emphasis on automation, performance, scalability, and use of open-source
    tools.'
  prefs: []
  type: TYPE_NORMAL
- en: According to Yaron Haviv, the Founder and CTO of Iguazio, “We evaluated various
    mechanisms which will allow us to scale deep learning frameworks with minimal
    developer effort and found that using the combination of Horovod with the MPI
    Operator over Kubernetes is the best tool for the job since it enable horizontal
    scalability, supports multiple frameworks such as TensorFlow and PyTorch and doesn’t
    require too much extra coding or the complex use of parameter servers.”
  prefs: []
  type: TYPE_NORMAL
- en: Iguazio have integrated the MPI Operator into its managed service offering and
    its fast data layer for maximum scalability, and work to simplify the usage through
    open source projects like [MLRun](https://github.com/mlrun/mlrun) (for ML automation
    and tracking). Check out [this blog post](https://towardsdatascience.com/gpu-as-a-service-on-kubeflow-fast-scalable-and-efficient-ml-c5783b95d192) with
    an example application that demonstrates Iguazio’s usage of the MPI Operator.
  prefs: []
  type: TYPE_NORMAL
- en: Polyaxon
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Polyaxon](https://polyaxon.com/) is a platform for reproducible and scalable
    machine learning on Kubernetes, it allows users to iterate faster on their research
    and model creation. Polyaxon provides a simple abstraction for data scientists
    and machine learning engineers to streamline their experimentation workflow, and
    provides a very cohesive abstraction for training and tracking models using popular
    frameworks such as Scikit-learn, TensorFlow, PyTorch, Apache MXNet, Caffe, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: “Several Polyaxon users and customers were requesting an easy way to perform
    an allreduce-style distributed training, the MPI Operator was the perfect solution
    to provide such abstraction. Polyaxon is deployed at several companies and research
    institutions, and the public docker hub has over 9 million downloads.”, says Mourad
    Mourafiq, the Co-founder of Polyxagon.
  prefs: []
  type: TYPE_NORMAL
- en: Community and Call for Contributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We are grateful for [over 28 individual contributors from over 11 organizations](https://github.com/kubeflow/mpi-operator/graphs/contributors),
    namely Alibaba Cloud, Amazon Web Services, Ant Financial, Bloomberg, Caicloud,
    Google Cloud, Huawei, Iguazio, NVIDIA, Polyaxon, and Tencent, that have contributed
    directly to MPI Operator’s codebase and many others who’ve filed issues or helped
    resolve them, asked and answered questions, and were part of inspiring discussions.
    We’ve put together a [roadmap](https://github.com/kubeflow/mpi-operator/blob/master/ROADMAP.md) that
    provides a high-level overview of where the MPI Operator will grow in future releases
    and we welcome any contributions from the community!
  prefs: []
  type: TYPE_NORMAL
- en: We could not have achieved our milestones without an incredibly active community.
    Check out our [community page](https://www.kubeflow.org/docs/about/community/) to
    learn more about how to join the Kubeflow community!
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/kubeflow/introduction-to-kubeflow-mpi-operator-and-industry-adoption-296d5f2e6edc).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[How Kubeflow Can Add AI to Your Kubernetes Deployments](/2020/02/kubeflow-ai-kubernetes-deployments.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Most Useful Machine Learning Tools of 2020](/2020/03/most-useful-machine-learning-tools-2020.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Easy, One-Click Jupyter Notebooks](/2019/07/easy-one-click-jupyter-notebooks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Ploomber vs Kubeflow: Making MLOps Easier](https://www.kdnuggets.com/2022/02/ploomber-kubeflow-mlops-easier.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How Has the Adoption of AI in Algorithmic Trading Affected the…](https://www.kdnuggets.com/2022/04/adoption-ai-algorithmic-trading-affected-finance-industry.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Promise of Edge AI and Approaches for Effective Adoption](https://www.kdnuggets.com/the-promise-of-edge-ai-and-approaches-for-effective-adoption)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SQL LIKE Operator Examples](https://www.kdnuggets.com/2022/09/sql-like-operator-examples.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How (Not) To Use Python''s Walrus Operator](https://www.kdnuggets.com/how-not-to-use-pythons-walrus-operator)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Cloud Storage Adoption is the Need of the Hour for Business](https://www.kdnuggets.com/2022/02/cloud-storage-adoption-need-hour-business.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
